
\chapter{Implementing VEP-based brain-computer interface}

The previous chapter discussed the biology of the brain and described the brain potential called \gls{SSVEP}. The aim of this chapter is to describe which kind of visual stimuli can be used to elicit \gls{SSVEP} and how to detect \gls{SSVEP} in a \gls{EEG} recording. This knowledge is needed to extract information from the \gls{EEG} recording and use this information to control a robot. In other words, this chapter will discuss how to implement a \gls{VEP}-based \gls{BCI}.

\section{Visual stimulus}
\label{sec:stimuli}

As discussed in section~\ref{sec:VEP}, it is possible to present multiple visual stimuli to a subject and detect, which stimulus is the subject looking at. It was also mentioned that computer monitor can be used to present visual stimuli. This section will discuss how stimuli with certain blinking frequency can be displayed on a computer screen. Research has shown that LCD screens produce more reliable \gls{SSVEP} response than \gls{LED}~\cite{lcd_lcd_led}. Another article, however, concludes that \glspl{BCI} that used \glspl{LED} as visual stimuli have achieved better performance~\cite{ssvep_stim}. But since using \glspl{LED} requires dedicated hardware, only the stimuli that can be presented by a computer monitor are discussed in this thesis.

A computer monitor has certain size, resolution and \gls{refresh rate}. Monitor resolution and size limit the size of the visual stimuli that can be used and the distance between the stimuli. Monitor \gls{refresh rate}, on the other hand, limits the presentation rate or blinking frequency of the stimulus that can be used. Research has shown that LCD screens produce more reliable \gls{SSVEP} response when using monitor \gls{refresh rate} for measuring the time between stimuli presentations rather than a timer~\cite{lcd_lcd_led}. Therefore, monitor \gls{refresh rate} is used for timing also in this thesis.

Monitor \gls{refresh rate} is the number of consecutive images or \glspl{frame} shown on the screen in a second, assuming that the \glspl{frame} are produces at least as fast as they can be displayed. A \gls{frame} is one of the images that compose the changing picture on the screen. Monitor with a \gls{refresh rate} of 60 Hz can display 60 \glspl{frame} per second. 

Often blinking one-coloured squares on one-coloured background are used as visual stimuli in \gls{SSVEP}-based \glspl{BCI}~\cite{ssvep_stim}. The squares may have symbols on them, for example letters or numbers. The stimuli of \gls{VEP}-based \gls{BCI} are also called \glspl{target} and the blinking of a \gls{target} is called \gls{flickering}.

In every \gls{frame} each \gls{target} can be in one of two \glspl{state}---displayed or not displayed. The \gls{state} of a \gls{target} can be switched only when the current \gls{frame} is replaced with the next \gls{frame}. The \gls{state} switches should be distributed as evenly as possible for the target frequency to be constant. Distributing the \gls{state} switches is easier with some frequencies than others. For example, if \gls{refresh rate} is 60 Hz, then
\begin{itemize}
	\item 10 Hz \gls{flickering} can be achieved by presenting the \gls{target} $\frac{60}{10}=6$ times slower than the \gls{refresh rate}. This means, that the \gls{target} has to be presented once in every 6 \glspl{frame}. Since 6 is an even number, 10 Hz \gls{flickering} can be achieved by changing the \gls{state} of the \gls{target} after every $\frac{6}{2} = 3$ \glspl{frame}. If this \gls{flickering} is plotted as a function of \gls{state} versus time as in figure~\ref{fig:flickering}, it can be seen that it produces a \gls{square wave}. In this thesis, the waveform produced by plotting the \gls{flickering} is called \gls{flickering waveform}.
	\item 12 Hz \gls{flickering} can be achieved by presenting the \gls{target} $\frac{60}{12}=5$ times slower than the \gls{refresh rate}. The \gls{target} has to be presented once in every 5 \glspl{frame}. Since 5 is an odd number, the amount of time the target is in on \gls{state} and in off \gls{state} cannot be equal. Therefore, the \gls{target} should be 3 \glspl{frame} in displayed \gls{state} and 2 \glspl{frame} in not displayed \gls{state} or the other way round. If representing this \gls{flickering} as a \gls{flickering waveform} as in figure~\ref{fig:flickering}, it can be seen that the waveform is a \gls{rectangular wave}.
	\item 11 Hz \gls{flickering} can be achieved by presenting the \gls{target} $\frac{60}{11}\approx 5.45$ times slower than the \gls{refresh rate}. This means, that the \gls{target} has to be presented once in every 5.45 \glspl{frame}. Since 5.45 is not a natural number, the \gls{flickering} will be irregular. 11 Hz target \gls{flickering} from the paper by Wang \textit{et al.}~\cite{11hz} is used as an example in figure~\ref{fig:flickering}. Although 11 Hz frequency produces irregular \gls{flickering}, it is still possible to detect \gls{SSVEP} elicited by it in the Emotiv EPOC recording~\cite{emotiv_11hz}.
\end{itemize}

\begin{figure}[h]
	\begin{subfigure}{\textwidth}
		\begin{tikztimingtable}[xscale=0.75, yscale=1.5, thick]
			10 Hz & [C] 10{3H 3L}\\
			11 Hz & [C] 11{2.73H 2.73L}\\
			12 Hz & [C] 12{2.5H 2.5L}\\
			\extracode
			\tablegrid[black!25,step=1]
		\end{tikztimingtable}
		\caption{Ideal target flickering}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\vspace{10pt}
		\begin{tikztimingtable}[xscale=0.75, yscale=1.5, thick]
			10 Hz & [C] 10{3H 3L}\\
			11 Hz & [C] 3H 3L 3H 2L 3H 3L 3H 2L 3H 3L 2H 3L 3H 3L 2H 3L 3H 3L 2H 3L 3H 2L\\
			12 Hz & [C] 12{3H 2L}\\
			\extracode
			\tablegrid[black!25,step=1]
		\end{tikztimingtable}
		\caption{Target flickering adjusted to refresh rate}
	\end{subfigure}
	\caption{Adjusting target flickering to 60 Hz refresh rate}
	\label{fig:flickering}
\end{figure}
A \gls{duty cycle} is used to characterise a \gls{rectangular wave}. \Gls{duty cycle} is the percentage of the amount of time the \gls{target} is in displayed \gls{state} in one period. If the target is in displayed \gls{state} for 2 \glspl{frame} and in not displayed \gls{state} for 3 \glspl{frame} in one period, then the \gls{duty cycle} of the \gls{rectangular wave} is $\frac{2}{2+3}\cdot 100\%=40\%$. \Gls{square wave} has a \gls{duty cycle} of 50\%. Research has shown that the \gls{SSVEP} elicited by a \gls{square wave} \gls{flickering} can be more accurately detected than those elicited by a \gls{rectangular wave} \gls{flickering}~\cite{ssvep_stim}.

The previous discussion was about blinking shapes or \gls{single graphic} stimuli. There is another type of \gls{SSVEP} stimuli called \gls{pattern reversal} stimuli that can also be displayed on a computer screen. \Gls{pattern reversal} stimuli is rendered by changing between two different patterns, for example alternating the colours of a chequerboard~\cite{ssvep_stim}. The main difference between \gls{single graphic} stimuli and \gls{pattern reversal} stimuli is that \gls{single graphic} stimuli elicits \gls{SSVEP} response after every two alterations, while \gls{pattern reversal} stimuli elicits \gls{SSVEP} response after every alteration~\cite{ssvep_stim}.

The fastest possible target frequency can be achieved by changing between on and off \glspl{state} of a target every time a new \gls{frame} is displayed. If the \gls{state} is changed at lower rate, the \gls{target} frequency will also be lower. Perhaps it is most reasonable to calculate the \gls{target} frequency with:
\begin{equation}
	f_{single\mbox{ }graphic} = \frac{n}{2T} \qquad f_{pattern\mbox{ }reversal} = \frac{n}{T}
\end{equation}
where $T$ is the period of the \gls{flickering waveform}, $n$ is the number of times the \gls{target} \gls{state} is switched in a time period of $T$ and $f$ is the \gls{flickering} frequency.

Therefore computer monitor can be used to present visual stimuli to a subject and monitor \gls{refresh rate} should be used to measure the time between stimuli presentations. Adjusting \gls{target} \gls{flickering} to the \gls{frame} changes can produce different \glspl{flickering waveform} for different \gls{flickering} frequencies. Since the \glspl{flickering waveform} are different, the \gls{SSVEP} response will also be different.

\section{Fourier analysis}
\label{sec:fourier}

As discussed in the previous section, the \gls{flickering} of a \gls{target} can produce different waveforms. This section will discuss how these waveforms and other signals can be represented by sums of simpler trigonometric functions. The study of this decomposition process is called Fourier analysis, named after Joseph Fourier, whose insight to model all functions by trigonometric series was a breakthrough in the field in 1807.

The following paragraph is based on the book by Hartmann~\cite{pure_tone}. The simpler trigonometric functions that a signal is decomposed into are \glspl{pure tone}---the waveforms that contain only one frequency. All other waveforms have at least two frequencies. \Gls{pure tone} waveforms are sine and cosine waves. Important property of a \gls{pure tone} is that linear operations do not change the shape of the \gls{pure tone} waveform.

\begin{figure}[h]
	\input{./tikz/pure_tone}
	\caption{Sine wave in time domain and its frequency component}
\end{figure}

The \glspl{pure tone} are used to represent all possible frequencies that a signal may contain. The decomposition process of a signal is called \gls{Fourier transform} and it is used to decompose a function of time into \glspl{pure tone} or \glspl{frequency component} that make it up. The function of time can be for example the \gls{EEG} recording represented as voltage versus time or the \gls{target} \gls{flickering} represented as \gls{state} versus time. \Gls{Fourier transform} converts signal from time domain or the function of time to frequency domain or the function of frequency. The representation of a time-domain signal in a frequency domain is called \gls{frequency spectrum}. \Gls{frequency spectrum} contains information about amplitude and phase of different frequencies. Therefore, \gls{frequency spectrum} can be presented as a function of frequency versus amplitude and phase.

To represent both amplitude and phase, complex numbers are used. Complex numbers can be represented as a pair of real numbers and therefore complex numbers can be used to represent two values. The amplitude and phase do not correspond to the real and imaginary part of the complex number but rather are related to the absolute value or the modulus and phase of the complex number. In this thesis the phase information from the \gls{frequency spectrum} is not used. There are, however, \glspl{BCI} that also use the phase information~\cite{MPCC}. Since only the information about amplitude is required in this thesis, it is possible to convert the \gls{frequency spectrum} into \gls{power spectral density}, which is represented as a function of amplitude squared or power versus frequency.

To conclude previous discussion, the amount of frequency $f$ present in a signal can be calculated by first calculating the \gls{frequency spectrum} with \gls{Fourier transform} and then taking modulus squared of the \gls{frequency spectrum}'s value at frequency $f$.

In digital devices, however, theoretical \gls{power spectral density} cannot be calculated. The measurement period would have to be infinitely long to a acquire the true \gls{power spectral density}~\cite{psd}. Therefore, spectral estimation is used. The modulus squared of frequency spectrum in a real-world application is called \gls{periodogram} and it is the estimation of the \gls{power spectral density}. There are other spectral estimation methods available.

Since digital devices work with discrete signals as discussed in section~\ref{sec:EEG_comparison}, in a real-world application discrete version of the \gls{Fourier transform} is used. The algorithm used to compute the discrete \gls{Fourier transform} is called \gls{FFT}. The \gls{frequency spectrum} calculated by \gls{FFT} is discrete---if the discrete real-valued input signal, as is the case with \gls{EEG} recording, has $N$ values then the output has $\frac{N}{2}$ values. This derives from the definition and symmetric property of the discrete \gls{Fourier transform}. %The higher the sampling rate and \gls{ADC} resolution of a recording device, the more accurate the recording and therefore the more accurate the frequency spectrum and the periodogram of the signal.

The signal that is recorded, however, may contain frequencies that are too high to be detected from the discrete signal. Theoretically, the \gls{sampling rate} of the device has to be more than two times higher than the highest frequency in the signal to reconstruct the continuous signal from the discrete signal and also to decompose the signal into \glspl{frequency component}. When looking it from different angle, the highest frequency that can be detected with a \gls{sampling rate} of $f$ is $\frac{2}{f}$ and it is called \gls{Nyguist frequency}. But since real-world application are imperfect, even higher \gls{sampling rate} is needed.

To conclude previous discussion, the \gls{frequency spectrum} calculated by \gls{FFT} from real-valued time-domain signal with $N$ values is defined at frequencies $\frac{1}{N}, \frac{2}{N}, \dots\frac{f}{2N}$. These frequencies are also called \glspl{frequency bin}. The length of a \gls{frequency spectrum} depends on the length of the signal from which it is calculated. The longer the input signal, the more \glspl{frequency bin} will be acquired.

Thus a time-domain signal can be decomposed into \glspl{pure tone} or sine and cosine waves using \gls{FFT}. \gls{FFT} calculates the \gls{frequency spectrum} of a time-domain signal. \Gls{frequency spectrum} can be converted to the estimation of \gls{power spectral density} which contains only the information about the amplitude of the \glspl{pure tone}. This knowledge can be used to detect \glspl{SSVEP} in \gls{EEG} recording.

%As already discussed in section~\ref{sec:EEG_comparison}, Emotiv EPOC has high internal sampling rate to filter out high frequencies

%Thus higher sampling rate and \gls{ADC} resolution lead to more accurate \gls{SSVEP} detection.

\section{Decomposition of target flickering}
\label{sec:decomposition}

This section will focus on the decomposition of the \glspl{flickering waveform}. The \gls{target} \gls{flickering} and decomposition process were discussed in section~\ref{sec:stimuli} and section~\ref{sec:fourier} respectively. As discussed in previous section, only sine and cosine waves are composed of single frequency. Other waveforms have more \gls{frequency component}. The \gls{frequency component} with lowest frequency is called \gls{fundamental} or the first \gls{harmonic} of the signal. \Gls{harmonic} of a signal is a \gls{frequency component} with frequency that is integer multiple of the \gls{fundamental} frequency of the signal. If the \gls{fundamental} frequency is $f$, then first, second are third \glspl{harmonic} have frequencies of $1f$, $2f$, $3f$ respectively.

It can be shown that a \gls{square wave} is composed of odd \glspl{harmonic}. If the \gls{flickering waveform} is a \gls{square wave} or a \gls{rectangular wave}, the \gls{fundamental} frequency of the waveform is the frequency of the stimuli presentation. Therefore, a \gls{square wave} with \gls{fundamental} frequency $f$ can be represented as a sum of sine waves
\begin{equation}
	\label{eq:square}
	square(t) = \sum_{n=1,3,5,\dots}^{\infty}\frac{1}{n} \sin(2\pi nft)
\end{equation}
where $square(t)$ is the \gls{flickering} represented as \gls{state} versus time, $\frac{1}{n}$ is the amplitude of a \gls{frequency component} and $f$ is the \gls{fundamental} frequency of the waveform. It can be seen that the \gls{fundamental} has the highest amplitude. \Gls{rectangular wave}'s \glspl{frequency component} depend on the \gls{duty cycle} of the wave. But as is the case with \glspl{square wave}, the \gls{fundamental} has the highest amplitude also in a \gls{rectangular wave}.

\begin{figure}[h]
	\input{./tikz/square_wave}
	\caption{Square wave components}
\end{figure}

Unfortunately, the \gls{SSVEP} response to \gls{target} \gls{flickering} does not contain only the frequencies that are present in the \glspl{frequency component} of the \gls{flickering waveform}---\gls{SSVEP} contains other frequencies too. However, the frequencies that are present in the \glspl{frequency component} of the \gls{flickering waveform} are more successfully elicited in \gls{SSVEP} response~\cite{square_sine}. It has even been reported that \gls{SSVEP} contains small \glspl{subharmonic} of the \gls{flickering waveform}~\cite{ssvep_response}. \Gls{subharmonic} of a signal is \gls{frequency component} with frequency $\frac{f}{n}$, where $f$ is the \gls{fundamental} frequency of the signal and $n$ is an integer.

\gls{SSVEP} contains \glspl{harmonic} of the \gls{flickering} frequency of the stimuli~\cite{ssvep_response}. Therefore, \gls{target} frequencies should be chosen so that the frequencies are not each other \glspl{harmonic} or \glspl{subharmonic}. Otherwise it might not be distinguishable which \gls{target} \gls{flickering} elicits which frequency in the \gls{SSVEP} response. For example, if 6 Hz and 12 Hz \gls{flickering} frequencies are both used, then 6 Hz \gls{flickering} also elicits 12 Hz frequency in the \gls{SSVEP} since 12 Hz is the second harmonic of 6 Hz \gls{flickering} and thus it cannot be distinguished whether a 12 Hz response is elicited by 6 Hz \gls{target} or 12 Hz \gls{target}.

Thus \gls{SSVEP} reflects certain properties of the visual stimulus. The frequencies that are present in the \gls{flickering waveform} are likely to be found also in the \gls{SSVEP} response, but other \glspl{frequency component} are present in the \gls{SSVEP} too~\cite{square_sine}. It is sufficient to detect only the \gls{frequency component} with the same frequency as the \gls{target} \gls{flickering}, but to improve the performance other \glspl{frequency component} should be detected too~\cite{harmonic_imrpovement}.

%Most important of these properties is that \gls{SSVEP} has a component with the same frequency as the visual stimulus. But that is not the only component frequency in \gls{SSVEP}. \gls{SSVEP} has other components too that are discussed in section~\ref{sec:fourier}. 

\section{Signal processing}

The aim of this section is to describe some digital signal processing techniques that can improve the performance of a \gls{SSVEP}-based \gls{BCI}.

\subsection{Zero padding and interpolation}

One problem that occurs when analysing \gls{power spectral density} is that the \gls{power spectral density} is discrete and therefore it might not have frequency bins at the exact values of interest. For example, if designing a \gls{BCI}, the frequencies of interest are the \glspl{target} frequencies or the \glspl{harmonic} of \glspl{target} \glspl{flickering}, since these are the \glspl{frequency component} of \gls{SSVEP} as discussed in section~\ref{sec:decomposition}.

\begin{figure}[h]
	\input{./tikz/zero_padding}
	\caption{Signal in time domain and its a}
\end{figure}

\Gls{interpolation} can be used to approximate the value between \glspl{frequency bin} to calculate the power of a \gls{target} frequency that does not correspond to any \gls{frequency bin}~\cite{cca_psda}. In general, \gls{interpolation} is used to construct a discrete signal between the discrete points or in other words to approximate the continuous signal from which the discrete values were extracted from. Therefore, \gls{interpolation} can also be used to approximate the \gls{EEG} recording if some of the values are lost in the process of sending values from the recording device to the computer.

Another possibility to solve the lack of \glspl{frequency bin} problem is to use \gls{zero padding}. \Gls{zero padding} means that zeros are added to the end of the discrete signal before performing \gls{FFT}. This results in more \glspl{frequency bin} in the \gls{power spectral density}, because the number of \glspl{frequency bin} depends on the length of the input signal as discussed in section~\ref{sec:fourier}. The only alteration in \gls{power spectral density} is that it has more \glspl{frequency bin} if calculated from a zero-padded signal.

\subsection{Trend in the signal}

Linear trend or constant increase or decrease of values in the \gls{EEG} recording can decrease the accuracy of detecting \glspl{SSVEP}. Since Fourier analysis is used to decompose the recorded signal into \glspl{frequency component}, the linear trend present in the recording will also be decomposed. The decomposed trend will not provide any useful information in the \gls{power spectral density} and it makes detecting the actual \gls{SSVEP} \glspl{frequency component} less accurate.

\begin{figure}[h]
	\input{./tikz/detrend_example}
	\caption{The frequency components' amplitudes of a signal with trend, without trend and of the trend itself}
\end{figure}

Removing trend from a signal is called detrending. The average value or the mean of a detrended signal is 0. therefore the signal should be detrended before performing \gls{FFT}.

\subsection{Windowing the signal}

When estimating the \gls{power spectral density} of a signal using \gls{FFT}, it has to be decided how long signal segments are used or in other words, how many samples have to be acquired before performing \gls{FFT}. Dividing the signal into segments can make the \gls{SSVEP} detection less accurate, because some periodic components might

\section{SSVEP-based BCI feature extraction methods}
\label{sec:SSVEP_detection}
The aim of this section is to describe two methods used to detect \glspl{SSVEP} in \gls{EEG} recording. These methods are called \gls{feature extraction} methods. This section describes the \gls{PSDA} and \gls{CCA} \gls{feature extraction} methods. These are the methods used in chapter~\ref{sec:SSVEP_BCI} to design an application for controlling a robot. % frequency-domain method and time-domain method

\subsection{Power spectral density analysis}

\Gls{PSDA} is widely used in \gls{SSVEP}-based \glspl{BCI}~\cite{bin2009cca}. This method is based on a \gls{power spectral density} estimation. \Gls{power spectral density} estimation method called \gls{periodogram} was discussed in section~\ref{sec:fourier}.

There are different ways how to use the \gls{power spectral density} estimation for \gls{feature extraction}. Some \glspl{BCI} use peak finding to find highest values in the \gls{power spectral density}~\cite{cca_lin}. Other \glspl{BCI} use a training session to find a threshold value that certain frequency's amplitude or power has to exceed in order to select the \gls{target} with corresponding frequency as user's choice. This method requires a training session during which the threshold values are determined, but \gls{SSVEP}-based \glspl{BCI} could be implemented without the need for a training session. Threshold values, however, can be used together with other methods. 

Common method in \glspl{BCI} that use \gls{PSDA} \gls{feature extraction} is to calculate \gls{SNR} or other values that are related to \gls{SNR}. For example, the ratio of frequency's power to the mean of adjacent frequency bins can be calculated~\cite{psda_snr1}
\begin{equation}
	\label{eq:snr1}
	\mbox{SNR}(f_k) = \frac{2P(f_k)}{P(f_{k+1})+P(f_{k-1})}
\end{equation}
where $\mbox{SNR}(f_k)$ is the \gls{SNR} of the frequency $f_k$, $P$ is the function representing \gls{power spectral density} and $f_1,\dots, f_k,\dots,f_m$ are the \glspl{frequency bin} of the \gls{periodogram} in increasing order. The \gls{SNR} as defined in equation~\ref{eq:snr1} can be calculated for every \gls{target} frequency and the \gls{target} that has the frequency with highest \gls{SNR} is assumed to be the user's choice. More than two adjacent frequency bins can be used to calculate the \gls{SNR}~\cite{psda_snr2}. Therefore, the equation~\ref{eq:snr1} can be generalised
\begin{equation}
	\mbox{SNR}(f_k) = \frac{nP(f_k)}{\sum_{i=1}^{n/2}P(f_{k-i})+P(f_{k+i})}
\end{equation}
where n is the number of adjacent frequency bins used. To generalise it even more, the ratio of the frequency's power to the whole \gls{power spectral density} or all the \glspl{frequency bin} can be calculated
\begin{equation}
	\mbox{SNR}(f_k) = \frac{P(f_k)}{\sum_i P(f_i)}
\end{equation}
This is the same as using normalised \gls{power spectral density} and comparing the normalised powers. It is also possible to just compare the powers of different \gls{target} frequencies and select the \gls{target} with highest power~\cite{cca_psda}. All these methods can use in addition to the \gls{target} frequency also its \glspl{harmonic}. In this case the sum of the \glspl{SNR} can be used
\begin{equation}
	\mbox{SNR}(f_k, 2f_k, \dots, hf_k)=\sum_{i=1}^{h}\mbox{SNR}(if_k)
\end{equation}
where $h$ is the number of \glspl{harmonic} used. Often three \glspl{harmonic} are used. After calculating the \gls{SNR} for all the \gls{target} frequencies and for their \glspl{harmonic} if necessary, the \gls{target} with highest \gls{SNR} or the highest sum of \glspl{SNR} can be selected as user's choice.

To design a \gls{SSVEP}-based \gls{BCI} that uses \gls{PSDA} \gls{feature extraction} method it is enough to use one of the methods described in this section. Often \glspl{SNR} or powers of different \glspl{target} are compared to determine the user's choice.

\subsection{Canonical correlation analysis}

\Gls{CCA} was first introduced by Harold Hotelling in 1936~\cite{cca_hotelling}. In 2001 \gls{CCA} was used to introduce a novel method for detecting neural activity in \gls{fMRI} data~\cite{cca_fmri}. Likewise, \gls{CCA} was introduced to \gls{EEG} recording analysis for the first time in 2007 by Lin~\textit{et al.}~\cite{cca_lin}. This section gives necessary background information and describes the method proposed by Lin~\textit{et al.}.

\subsubsection{Comparing two signals}

Mathematically \gls{EEG} recordings can be modelled using random variables. This interpretation is necessary to make mathematical statements about the calculated statistics, such as the mean and covariance. One specific \gls{EEG} recording can be viewed as a data sample or a set of data collected from the continuous signal. Therefore, \gls{EEG} recording can be represented as a sequence of recorded values $\mathbf{x}=(x_1, x_2, x_3, \dots, x_n)$. 

As was the case with \gls{power spectral density}, the statistics calculated from a data sample are not the exact parameters for the whole \gls{EEG} signal. Statistics are used to estimate the theoretical values. The sample mean of a data set, for example of an \gls{EEG} recording can be calculated with
\begin{equation}
	m(\mathbf{x}) = \frac{1}{n}\sum_{i=1}^{n}x_i
\end{equation}

To measure the similarity of two \gls{EEG} recordings, sample covariance can be used. Sample covariance measures how much two data sets change together. Thus for two \gls{EEG} recordings $\mathbf{x}=(x_1,\dots,x_n)$ and $\mathbf{y}=(y_1, \dots, y_n)$ the sample covariance is
\begin{equation}
	\label{eq:cov}
	q(\mathbf{x},\mathbf{y}) = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-m(\mathbf{x}))(y_i-m(\mathbf{y}))
\end{equation}
The addend $(x_i-m(\mathbf{x}))(y_i-m(\mathbf{y}))$ in the equation~\ref{eq:cov} is positive, if both signals change similarly at the corresponding time point or in other words, if both signals are above or below their means at the same time. The addend is negative, if the signals change in the opposite way or in other words, if one signal is above its mean and the other is below its mean.

Sample covariance can be normalised so that the result will be between 1 and -1. Normalised covariance is called sample correlation. Sample correlation can be calculated with
\begin{equation}
	\label{eq:corr}
	r(\mathbf{x},\mathbf{x}) = \frac{q(\mathbf{x},\mathbf{y})}{\sqrt{q(\mathbf{x},\mathbf{x})q(\mathbf{y},\mathbf{y})}}
\end{equation}
There is somewhat different correlation defined for time-domain signals, called cross-correlation. Cross-correlation takes also into account the possible shift in time between the signals. For example, the correlation of sine and cosine calculated with equation~\ref{eq:corr} is 0, which means that according to equation~\ref{eq:corr} sine and cosine are uncorrelated despite the fact that sine and cosine waves are actually very similar. Cross-correlation is the function of the measure of similarity versus time lag between the signals
\begin{equation}
	\label{eq:cross_corr}
	(\mathbf{x}\star \mathbf{y})(t)=\sum_{i}x_iy_{i+t}
\end{equation}

The equations \ref{eq:corr} and \ref{eq:cross_corr} can be used to measure the similarity between two \gls{EEG} recordings. Measuring the similarity between two signals can be used as \gls{feature extraction} method called template matching. The template matching method requires a training session to acquire templates that can be later compared to \gls{EEG} recording when a subject is using the \gls{BCI}. Each template shows the state of the brain when the subject is watching certain \gls{target}. If the template acquired during training session and the current recording are similar enough, the target corresponding to the matching template will be selected.

\subsubsection{Reference signals}

\gls{CCA} method does not use templates as discussed in the previous section. \gls{CCA} uses sine and cosine waves instead of templates in \gls{SSVEP}-based \glspl{BCI}. \gls{CCA} is a statistical method used to measure the similarity between two sets of signals or two sets of data samples. Therefore, \gls{CCA} can be used to measure the similarity between multichannel \gls{EEG} recording and multiple reference signals. In contrast the equation~\ref{eq:corr} can be used to measure the similarity between only two signals or two data samples.

In \gls{SSVEP}-based \glspl{BCI} one set of data is the multichannel \gls{EEG} recording. For example, if recording data with electrodes located in O1 and O2, the recorded data can be represented as a vector $\mathbf{X}=(\mathbf{x}_{O1}, \mathbf{x}_{O2})$. The second set of data contains the \glspl{harmonic} of a \gls{target} frequency. There is different set of reference signals for every \gls{target}, because \glspl{target} have different frequencies. Both sine and cosine waves are used in the second data set
\begin{equation}
	\label{eq:cca_ref}
	\mathbf{Y}=\begin{pmatrix}
		y_{sin1}(t)\\
		y_{cos1}(t)\\
		y_{sin2}(t)\\
		y_{cos2}(t)\\
		y_{sin3}(t)\\
		y_{cos3}(t)\\
	\end{pmatrix}=\begin{pmatrix}
		\sin(2\pi 1ft)\\
		\cos(2\pi 1ft)\\
		\sin(2\pi 2ft)\\
		\cos(2\pi 2ft)\\
		\sin(2\pi 3ft)\\
		\cos(2\pi 3ft)\\
	\end{pmatrix}
\end{equation}

In the method proposed by Lin \textit{et al.}~\cite{cca_lin} three harmonics are used as in equation~\ref{eq:cca_ref}. The reason why both sine and cosine waves are used is that the phases of the \gls{SSVEP} components are not known. As already mentioned in the previous section, sine and cosine waves are uncorrelated according to equation~\ref{eq:corr}. Therefore, using both sine and cosine as reference signals gives optimal minimum correlation of $\frac{1}{\sqrt{2}}$ between a reference signal and an ideal \gls{SSVEP} component.

The optimal minimum correlation means that if \gls{SSVEP} component with the same frequency and amplitude as the reference signals but with different phase is compared to both sine and cosine reference, then the absolute value of the maximum of these two correlations will be no less than $\frac{1}{\sqrt{2}}\approx 0.707$. This can be seen when calculating the cross-correlation of the \gls{SSVEP} component with both signals and taking the absolute value of these correlations. See figure~\ref{fig:cross_corr} for example. It is possible to take the absolute value of the correlations because \gls{CCA} treats correlation and anticorrelation similarly.

\begin{figure}[h]
	\label{fig:cross_corr}
	\input{./tikz/cross_correlation}
	\caption{Cross}
\end{figure}

Thus if there is \gls{SSVEP} component that corresponds to \gls{target} frequency or its \gls{harmonic}, then there is positive correlation between the \gls{SSVEP} component and at least one of the reference signals.

\subsubsection{Comparing two sets of signals}

The covariance between two sets of data samples $\mathbf{X}=(\mathbf{x}_1,\dots\mathbf{x}_n)$ and $\mathbf{Y}=(\mathbf{y}_1,\dots,\mathbf{y}_m)$ can be calculated by summing up the covariances of all the possible combinations of two data samples:
\begin{equation}
	q(\mathbf{X}, \mathbf{Y}) = \sum_{i=1}^{n}\sum_{j=1}^{n}q(\mathbf{x}_i, \mathbf{y}_j)
\end{equation}
where $q(\mathbf{x}_i, \mathbf{y}_j)$ is calculated using the equation~\ref{eq:cov}.

The canonical correlation, however, is not just the correlation between two sets of data samples, but the correlation between the linear combination of one set and the linear combination of the other set of data samples. Linear combinations of $\mathbf{X}$ and $\mathbf{Y}$ are
\begin{equation*}
	\mathbf{U} = a_1\mathbf{x}_1 + a_2\mathbf{x}_2 + a_3\mathbf{x}_3 + \dots + a_n\mathbf{x}_n
\end{equation*}
\begin{equation*}
	\mathbf{V} = b_1\mathbf{y}_1 + b_2\mathbf{y}_2 + b_3\mathbf{y}_3 + \dots + b_m\mathbf{y}_m
\end{equation*}

\gls{CCA} seeks linear combinations $\mathbf{U}$ and $\mathbf{V}$ that have the maximum correlation among all the possible linear combinations of $\mathbf{X}$ and $\mathbf{Y}$
\begin{equation}
	p = \frac{q(\mathbf{U}, \mathbf{V})}{\sqrt{q(\mathbf{U},\mathbf{U})q(\mathbf{V},\mathbf{V})}}
\end{equation}
This correlation is called canonical correlation. The pair $(\mathbf{U}, \mathbf{V})$ is called the first pair of canonical variates. It is possible to find up to $\min(m, n)$ pairs of canonical variates, but in the method proposed by Lin \textit{et al.}~\cite{cca_lin} only the first pair of canonical variates is used.

Thus the canonical correlation between the \gls{EEG} recording and every set of reference signals is calculated. Every target has different set of reference signals as already mentioned in the previous section and therefore target whose set of reference signals has the highest canonical correlation with \gls{EEG} recording will be selected as user's choice.
